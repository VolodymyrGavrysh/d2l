{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff65311d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# started 04/07/22\n",
    "from mxnet import np, npx, autograd\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb2dd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.arange(20).reshape(4,5)\n",
    "print(d)\n",
    "# norms - > how big vertor is? L2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e9d75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of gradient descent\n",
    "x = np.arange(4)\n",
    "# allocate memory for the gradient\n",
    "x.attach_grad()\n",
    "print(x.grad)\n",
    "with autograd.record():\n",
    "    y = 2 * np.dot(x, x)\n",
    "y.backward() # calculate the gradient\n",
    "print(x.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6586c46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  joint, marginal, and conditional probability\n",
    "\n",
    "# conditional probabilities \n",
    "# P(A|B) = P(B|A) * P(A) / P(B)\n",
    "# marginal probability // Відособлений розподіл\n",
    "# P(B) = SUM ( P(A|B) ) \n",
    "\n",
    "# Joint Probability: Probability of events A and B.\n",
    "# Marginal Probability: Probability of event X=A given variable Y.\n",
    "# Conditional Probability: Probability of event A given event B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8525f99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analytical solution of loss function\n",
    "# W = (XT X)-1 * XT * y (closed form / analytical solution )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4c8c9e",
   "metadata": {},
   "source": [
    "# model the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fb5fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import mxnet as d2l\n",
    "from mxnet import autograd, gluon, np, npx\n",
    "from mxnet.gluon import nn\n",
    "from mxnet import init\n",
    "from mxnet import gluon\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2a1088",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_w = np.array([2, -3.4])\n",
    "true_b = 4.2\n",
    "features, labels = d2l.synthetic_data(true_w, true_b, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81d030e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_array(data_arrays, batch_size, is_train=True): #@save\n",
    "    \"\"\"Construct a Gluon data iterator.\"\"\"\n",
    "    dataset = gluon.data.ArrayDataset(*data_arrays)\n",
    "    return gluon.data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "\n",
    "batch_size = 10\n",
    "data_iter = load_array((features, labels), batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e004e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model \n",
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(1))\n",
    "# init W\n",
    "net.initialize(init.Normal(sigma=0.01))\n",
    "print(net)\n",
    "# loss\n",
    "loss = gluon.loss.L2Loss()\n",
    "# SGD\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.01})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b359ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "epoch = 3\n",
    "for i in range(epoch):\n",
    "    for data, label in data_iter:\n",
    "        with autograd.record():\n",
    "            l = loss(net(data), lable)\n",
    "        l.backward()\n",
    "        trainer.step(batch_size)\n",
    "    print(f'epoch {epoch + 1}, loss {l.mean().asnumpy():f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92f6124",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = net[0].weight.data()\n",
    "print(f'error in estimating w: {true_w - w.reshape(true_w.shape)}')\n",
    "b = net[0].bias.data()\n",
    "print(f'error in estimating b: {true_b - b}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117cf741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # soft max fucntion for classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcdfd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from d2l import mxnet as d2l\n",
    "from mxnet import gluon, init, npx\n",
    "import mxnet as mx\n",
    "import sys\n",
    "from mxnet.gluon import nn\n",
    "npx.set_np()\n",
    "d2l.use_svg_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccf53c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = mx.gluon.data.vision.datasets.FashionMNIST(train=True)\n",
    "mnist_test = mx.gluon.data.vision.datasets.FashionMNIST(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf0d52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `ToTensor` converts the image data from uint8 to 32-bit floating point. It\n",
    "# divides all numbers by 255 so that all pixel values are between 0 and 1\n",
    "transformer = gluon.data.vision.transforms.ToTensor() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76326fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter = gluon.data.DataLoader(mnist_train.transform_first(transformer), batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "test_iter = gluon.data.DataLoader(mnist_test.transform_first(transformer), batch_size=batch_size, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515c39bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_iter:\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b957a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset has 10 classes; each image has 28 * 28 pixels size, so we got the vecor of size 784\n",
    "number_inputs = 784\n",
    "number_outputs = 10\n",
    "# set the NN model\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(10)) # 10 classes \n",
    "net.initialize(init.Normal(sigma=0.01)) # set weights with intial values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c360d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96b1d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 6\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9d7940",
   "metadata": {},
   "source": [
    "* Multilayer Perceptrons\n",
    "* Linear Models May Go Wrong - so hidden layers increase complexity\n",
    "* With deep neural networks, we used observational data to jointly learn both a representation via hidden \n",
    "* layers and a linear predictor that acts upon that representation.\n",
    "\n",
    "* Incorporating Hidden Layers to Nonlinear relations \n",
    "* activation function\n",
    "* MLPs are universal approximators\n",
    "* ReLU Function\n",
    "* sigmoid(x) = 1 / (1 + exp(−x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9e3c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement multilayer Perceptron\n",
    "from d2l import mxnet as d2l\n",
    "from mxnet import gluon, init, npx\n",
    "import mxnet as mx\n",
    "from mxnet.gluon import nn\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(256, activation='relu'), nn.Dense(10)) # 256 units of 10 hidden layers with activation relu\n",
    "net.initialize(init.Normal(sigma=0.01))\n",
    "print(net.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ff6e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.train_ch3(\n",
    "    net,\n",
    "    train_iter,\n",
    "    train_iter,\n",
    "    gluon.loss.SoftmaxCrossEntropyLoss(), \n",
    "    10, # epoch\n",
    "    gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1})\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bc58d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model complexity\n",
    "## learn polynomial degrees \n",
    "\n",
    "## Prepare data\n",
    "from d2l import mxnet as d2l\n",
    "from mxnet import gluon, np, npx, init\n",
    "from mxnet.gluon import nn\n",
    "import math\n",
    "npx.set_np()\n",
    "\n",
    "max_degree = 20 # Maximum degree of the polynomial\n",
    "n_train, n_test = 100, 100 # Training and test dataset sizes\n",
    "true_w = np.zeros(max_degree) # Allocate lots of empty space\n",
    "true_w[0:4] = np.array([5, 1.2, -3.4, 5.6])\n",
    "features = np.random.normal(size=(n_train + n_test, 1))\n",
    "np.random.shuffle(features)\n",
    "\n",
    "poly_features = np.power(features, np.arange(max_degree).reshape(1, -1))\n",
    "for i in range(max_degree):\n",
    "    poly_features[:, i] /= math.gamma(i + 1) # `gamma(n)` = (n-1)!\n",
    "    \n",
    "# Shape of `labels`: (`n_train` + `n_test`,)\n",
    "labels = np.dot(poly_features, true_w)\n",
    "labels += np.random.normal(scale=0.1, size=labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_features, test_features, train_labels, test_labels, num_epochs=400):\n",
    "     loss = gluon.loss.L2Loss()\n",
    "     net = nn.Sequential()\n",
    "     # Switch off the bias since we already catered for it in the polynomial features\n",
    "     net.add(nn.Dense(1, use_bias=False))\n",
    "     net.initialize()\n",
    "     batch_size = min(10, train_labels.shape[0])\n",
    "     train_iter = d2l.load_array((train_features, train_labels), batch_size)\n",
    "     test_iter = d2l.load_array((test_features, test_labels), batch_size, is_train=False)\n",
    "     trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1})\n",
    "     animator = d2l.Animator(xlabel='epoch', ylabel='loss', yscale='log', \n",
    "                    xlim=[1, num_epochs], ylim=[1e-3, 1e2], legend=['train', 'test'])\n",
    "     for epoch in range(num_epochs):\n",
    "          d2l.train_epoch_ch3(net, train_iter, loss, trainer)\n",
    "          if epoch == 0 or (epoch + 1) % 20 == 0:\n",
    "               animator.add(epoch + 1, (evaluate_loss(net, train_iter, loss), evaluate_loss(net, test_iter, loss)))\n",
    "     print('weight:', net[0].weight.data().asnumpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edf53bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(poly_features[:n_train, :4], poly_features[n_train:, :4],\n",
    "            labels[:n_train], labels[n_train:])\n",
    "# [5, 1.2, -3.4, 5.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1eb048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick all the dimensions from the polynomial features\n",
    "train(poly_features[:n_train, :], \n",
    "    poly_features[n_train:, :], \n",
    "    labels[:n_train], \n",
    "    labels[n_train:], num_epochs=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102d323f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight decay L2 - ridge regression algorithm / \n",
    "#  select models where features distributed more evenly / shrink the size of W towards 0\n",
    "\n",
    "# L1 - lasso regression / lead to models with some set of features -> feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a5982f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DropOut \n",
    "\n",
    "## Naively applied, linear models do not take into account interactions among features.\n",
    "#  For every feature, a linear model must assign either a positive or a negative weight, ignoring context.\n",
    "\n",
    "# ... neural network overfitting is characterized by a state in which each layer relies on a specifc\n",
    "# pattern of activations in the previous layer, calling this condition co-adaptation. Dropout, they\n",
    "# claim, breaks up co-adaptation just as sexual reproduction is argued to break up co-adapted genes.\n",
    "\n",
    "# Typically, we disable dropout at test time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085a34fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# J as the objective function\n",
    "\n",
    "# Frobenius norm of the matrix is simply the L2 norm applied after flattening the matrix into a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65456a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation -> calculating the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dced7588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.8 Numerical Stability and Initialization\n",
    "## vamishing gradients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d42fa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2.1 Parameter Access\n",
    "print(net[1].params)\n",
    "print(net[1].bias)\n",
    "print(net[1].bias.data())\n",
    "net[1].weight.grad()\n",
    "# all params\n",
    "print(net[0].collect_params()) \n",
    "print(net.collect_params())\n",
    "\n",
    "net.collect_params()['dense1_bias'].data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b788c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in Initialization\n",
    "net.initialize(init=init.Normal(sigma=0.01), force_reinit=True)\n",
    "net[0].weight.data()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fea5fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04363528, 0.09803008],\n",
       "       [0.03606788, 0.08567319]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5.4.2 Layers with Parameters\n",
    "from mxnet import gluon, np, npx, init\n",
    "from mxnet.gluon import nn\n",
    "npx.set_np()\n",
    "\n",
    "class NNparametrixed(nn.Block):\n",
    "    def __init__(self, input, output, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.ww = self.params.get('ww', shape=(output, input))\n",
    "        self.bb = self.params.get('bb', shape=(input,))\n",
    "    def forward(self, X):\n",
    "        return npx.relu(\n",
    "            np.dot(X, self.ww.data(ctx=X.ctx)) + self.bb.data(ctx=X.ctx)\n",
    "        )\n",
    "n = NNparametrixed(2, 2)\n",
    "n.initialize()\n",
    "n(np.random.uniform(size=(2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e8a21ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(cpu(0), gpu(0), gpu(1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mxnet import np, npx\n",
    "from mxnet.gluon import nn\n",
    "npx.set_np()\n",
    "npx.cpu(), npx.gpu(), npx.gpu(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "180b5f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npx.num_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4e4e60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "daea455f8f2fa5fd1325ecdf70d39c36f94abce8f384f5076330d1881fe6b777"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
