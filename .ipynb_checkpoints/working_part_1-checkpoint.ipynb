{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import autograd, np, npx\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 5]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one = [1,2,3,4,5]\n",
    "two = [1,4,6,7,8]\n",
    "list(set(one) - set(two))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 6, 7]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(two) - set(one))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(one) & set(two))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(one) | set(two))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "770"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items_not_in_old_from_new = list(set(data.new.values) - set(data.old.values))\n",
    "len(items_not_in_old_from_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(19)\n",
    "x.attach_grad()\n",
    "x.grad\n",
    "with autograd.record():\n",
    "    y = x ** 2 + 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()\n",
    "x.grad\n",
    "x.grad == x*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " plt.plot(x.grad, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while np.linalg.norm(b) < 1000:\n",
    "        b = b * 2\n",
    "        if b.sum() > 0:\n",
    "            c = b\n",
    "        else:\n",
    "            c = 100 * b\n",
    "    return c\n",
    "\n",
    "f(np.random.normal())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import mxnet\n",
    "\n",
    "x = np.linspace(-5, 6)\n",
    "\n",
    "x.attach_grad()\n",
    "\n",
    "with autograd.record():\n",
    "    y = np.sin(x)\n",
    "\n",
    "y.backward()\n",
    "\n",
    "mxnet.plot(x, (y, x.grad), legend=['sin(x)', 'grad x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import mxnet as d2l\n",
    "from mxnet import np, npx\n",
    "import random\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.random.multinomial(10, [1/6]*6, size=500)\n",
    "\n",
    "cum_counts = counts.astype(np.float32).cumsum(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimates = cum_counts / cum_counts.sum(axis=1, keepdims=True)\n",
    "\n",
    "d2l.set_figsize((6, 4.5))\n",
    "for i in range(6):\n",
    "    d2l.plt.plot(estimates[:, i].asnumpy())\n",
    "d2l.plt.axhline(0.167)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.arange(20).reshape(5, 4)\n",
    "\n",
    "A / A.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "dd = np.random.randint(0, 10, size=(2, 3, 4))\n",
    "print(dd)\n",
    "print(dd.sum(axis=0).shape)\n",
    "print(dd.sum(axis=1).shape)\n",
    "print(dd.sum(axis=2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate array with shape\n",
    "x = np.random.randint(0, 10, size=(3, 4))\n",
    "x1 = np.random.randint(10, 15, size=(4))\n",
    "# dot \n",
    "X = np.dot(x, x1)\n",
    "\n",
    "x.shape[1] == x1.shape[0] and X.shape[0] == x.shape[0]\n",
    "\n",
    "x, x1, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix\n",
    "\n",
    "A = np.ones(shape=(11, 12))\n",
    "B = np.ones(shape=(12, 10))\n",
    "C = np.dot(A, B)\n",
    "print(C.shape)\n",
    "\n",
    "A.shape[1] == B.shape[0] and C.shape[0] == A.shape[0] and C.shape[1] == B.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 2.4\n",
    "x = np.arange(-3, 3, 0.1)\n",
    "plt.plot(x, x**3+1/x)\n",
    "plt.plot(x, x * 4 - 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "from matplotlib.pyplot import figure\n",
    "import math\n",
    "\n",
    "figure(figsize=(8, 6), dpi=80)\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "\n",
    "X = np.arange(-5, 5, 0.25)\n",
    "Y = np.arange(-5, 5, 0.25)\n",
    "\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "\n",
    "R = (6*X + 5*math.e**X + 5*Y*math.e**Y)\n",
    "\n",
    "surf = ax.plot_surface(X, Y, R, cmap=cm.coolwarm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import autograd, np, npx\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.attach_grad()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with autograd.record():\n",
    "    y = 2 * np.dot(x, x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detaching computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with autograd.record():\n",
    "    y = x * x\n",
    "    print(y)\n",
    "    u = y.detach() # New variable u that has the same value as y but no info how it is computed \n",
    "    print(u)\n",
    "    # partial derivities with respect to x where u is constant\n",
    "    z = u * x\n",
    "    print(z)\n",
    "    \n",
    "z.backward()\n",
    "print(z, u, y, x)\n",
    "x.grad == u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()\n",
    "x.grad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "from mxnet import autograd, np, npx\n",
    "npx.set_np()\n",
    "\n",
    "def f(a):\n",
    "    b = a * 2\n",
    "    while numpy.linalg.norm(b) < 1000:\n",
    "        b = b * 2\n",
    "    if b > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return b\n",
    "\n",
    "f(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.normal()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.attach_grad()\n",
    "\n",
    "with autograd.record():\n",
    "    d = f(a)\n",
    "    print(d)\n",
    "    s = d.detach()\n",
    "    print(s)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.grad, d, a, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.grad, d, a, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5.5 task\n",
    "from d2l import mxnet\n",
    "import matplotlib.pyplot as plt\n",
    "from mxnet import autograd, np, npx\n",
    "npx.set_np()\n",
    "\n",
    "def derivative(x, y):\n",
    "    gradients = np.zeros(y.shape[0])\n",
    "    for num in range(y.shape[0] - 1):\n",
    "        gradients[num + 1] = (y[num+1] - y[num]) / (x[num+1] - x[num])\n",
    "    return gradients\n",
    "\n",
    "values = np.linspace(-np.pi, np.pi, 100)\n",
    "\n",
    "values.attach_grad()\n",
    "\n",
    "with autograd.record():\n",
    "    f = np.sin(values)\n",
    "    \n",
    "f.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosx, cosx_ = values.grad, derivative(values, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mxnet.plot(values, (f, cosx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mxnet.plot(values, (f, cosx_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proba\n",
    "n_times = 1000\n",
    "dice_with_teoretical_proba = [1.0 / 6] * 6\n",
    "print(dice_with_teoretical_proba)\n",
    "\n",
    "rres = np.random.multinomial(n_times, dice_with_teoretical_proba) / n_times\n",
    "print(rres)\n",
    "print('MSE')\n",
    "print(sum((rres - np.array(dice_with_teoretical_proba)) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 2.6\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def plot_distrib(distribution, sample, mx, nx):\n",
    "    plt.figure(figsize=(4, 3), dpi=80)\n",
    "    sns.distplot(distribution, color='red', label='True distribution')\n",
    "    sns.distplot(sample, color='blue', label='True distribution')\n",
    "    plt.title(f'True and Sampel distr. with dist size {mx} and sample size {nx}')\n",
    "    plt.legend(loc='right')\n",
    "    plt.show();\n",
    "\n",
    "nx = [50, 100]\n",
    "mx = [300, 500]\n",
    "for m in mx:\n",
    "    for n in nx:\n",
    "        distribution = np.random.normal(size=m)\n",
    "        sample = np.random.choice(distribution, size=n)\n",
    "        plot_distrib(distribution, sample, m, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.mean([np.max(np.random.randint(1, 7, size=2)) for i in range(int(10))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([np.max(np.random.randint(1, 7, size=2)) for i in range(int(10e4))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 normal dist\n",
    "from d2l import mxnet as d2l\n",
    "import math\n",
    "from mxnet import np\n",
    "\n",
    "def normal(x, mu, sigma):\n",
    "    p = 1 / math.sqrt(2 * math.pi * sigma**2)\n",
    "    return p * np.exp(-0.5 / sigma**2 * (x - mu)**2)\n",
    "\n",
    "val = ([0,1], [0,2], [1, 3], [1,2])\n",
    "\n",
    "data = np.arange(-5, 5, 0.01)\n",
    "\n",
    "d2l.plot(data, [normal(data, i, y) for i, y in val], figsize=(4.5, 2.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.plot(data, [normal(data, 0, i) for i in np.arange(0.2, 1, 0.2)], figsize=(10, 10), \\\n",
    "        legend=[f'mean 0, std {sigma}' for sigma in np.arange(0.2, 1, 0.2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(0.1, 1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2 from D2L p3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data, target, feature_names = load_boston()['data'], load_boston()['target'], load_boston()['feature_names']\n",
    "\n",
    "X = pd.DataFrame(data, columns=feature_names).values\n",
    "y = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use @ instead dot\n",
    "assert (np.dot(X, np.ones(X.shape[1])) == X @ np.ones(X.shape[1])).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheta = np.ones(X.shape[1])\n",
    "alpha = 0.0001\n",
    "EPOCH = 12\n",
    "cost_epoch = []\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    # derivation per epoch\n",
    "    derivation_epoch = []\n",
    "    for row in range(X.shape[1]):\n",
    "        all_guess = []\n",
    "        derivation = 0\n",
    "        cost = 0\n",
    "        for sample in range(len(X)):\n",
    "            guess = 0\n",
    "            for feature in range(X.shape[1]):\n",
    "                guess += pheta[feature] * X[sample, feature]\n",
    "            # loss over one feature\n",
    "            cost_i = (guess - y[sample]) ** 2\n",
    "            cost += cost_i\n",
    "            # guess per feature\n",
    "            all_guess.append(guess)\n",
    "            # derivation\n",
    "            derivation_ = (guess - y[sample]) * X[sample, row]\n",
    "            derivation += derivation_\n",
    "        # derivation over all rows\n",
    "        derivation = (1/len(X)) * derivation\n",
    "        derivation_list.append(derivation)\n",
    "        # cost over all rows\n",
    "        cost = (1/(2* len(X))) * cost\n",
    "        \n",
    "    # all derivation by vector\n",
    "    derivation_sum_vector = (1/ len(X)) * np.transpose(X) @ (X @ pheta - y)\n",
    "    derivation_list.append(derivation_sum_vector)\n",
    "    ### PER EPOCH \n",
    "    pheta = pheta - alpha * (derivation_sum_vector)\n",
    "    # all cost by vector\n",
    "    cost_vector = np.transpose(X @ pheta - y) @ (X @ pheta - y) * (1/(2 * len(X)))\n",
    "    print(cost_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0.0005 # https://towardsdatascience.com/vectorization-implementation-in-machine-learning-ca652920c55d\n",
    "theta = np.ones(n)\n",
    "cost_list = []\n",
    "for i in range(10):\n",
    "    \n",
    "    theta = theta - a*(1/ len(X))*np.transpose(X)@(X@theta - y)\n",
    "           \n",
    "#     cost_val = (1/(2*m))*np.transpose((X@theta - y))@(X@theta - y)\n",
    "    print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2\n",
    "%matplotlib inline\n",
    "from d2l import mxnet as d2l\n",
    "from mxnet import autograd, np, npx\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(weights, bias, num_examples):\n",
    "    X = np.random.normal(loc=0, scale=1, size=(num_examples, len(weights)))\n",
    "    y = np.dot(X, weights) + bias\n",
    "    y += np.random.normal(0, 0.01, y.shape)\n",
    "    return X, y.reshape((-1, 1))\n",
    "\n",
    "true_w = np.array([2, -3.4])\n",
    "true_b = 4.2\n",
    "features, labels = generate_data(true_w, true_b, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=features[:, 0], y=labels, c='orange')\n",
    "plt.scatter(x=features[:, 1], y=labels, c='red')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_batch(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    assert batch_size <= num_examples\n",
    "    indices = list(range(num_examples))\n",
    "    random.shuffle(indices) # reshuffle \n",
    "    for index in range(0, num_examples, batch_size):\n",
    "#         print(f'The number of batches is {int(num_examples / batch_size)}')\n",
    "        batch_indices = np.array(indices[index : min(index + batch_size, num_examples)])\n",
    "    yield features[batch_indices], labels[batch_indices]\n",
    "    \n",
    "# test\n",
    "test_batch_size = 100\n",
    "assert [(len(i), len(g)) for i, g in read_data_batch(test_batch_size, features, labels)] \\\n",
    "                            == [(test_batch_size, test_batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(X, w, bias):\n",
    "    return np.dot(X, w) + bias\n",
    "\n",
    "def loss(y_pred, y):\n",
    "    return ((y_pred - y.reshape(y_pred.shape)) ** 2) / 2\n",
    "\n",
    "def sgd(pheta, lr, batch_size):\n",
    "    for param in pheta:\n",
    "        param[:] = param - (lr * param.grad) / batch_size\n",
    "#     print(pheta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set W and b and attache grads\n",
    "w = np.random.normal(loc=0, scale=0.01, size=(features.shape[1], 1))\n",
    "w = np.zeros(shape=(features.shape[1], 1))\n",
    "\n",
    "b = np.zeros(1)\n",
    "w.attach_grad()\n",
    "b.attach_grad()\n",
    "\n",
    "lr = 0.03\n",
    "EPOCH = 10\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    # read data\n",
    "    for X, y in read_data_batch(batch_size, features, labels):\n",
    "        with autograd.record():\n",
    "            l = loss(linear_model(X, w, b), y)\n",
    "            print(l.shape)\n",
    "        l.backward()\n",
    "        sgd([w, b], lr, batch_size)\n",
    "        \n",
    "    train = loss(linear_model(features, w, b), labels)\n",
    "    \n",
    "    print(f'Epoch # is {epoch} and loss is {float(train.mean()):f}')\n",
    "print()\n",
    "print(w, b)\n",
    "print()\n",
    "print(true_w, true_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use lib\n",
    "\n",
    "from d2l import mxnet as d2l\n",
    "from mxnet import autograd, gluon, np, npx\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_w = np.array([2, -3.4])\n",
    "true_b = 4.2\n",
    "features, labels = d2l.synthetic_data(true_w, true_b, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "\n",
    "data, target, feature_names = load_boston()['data'], load_boston()['target'], load_boston()['feature_names']\n",
    "data = np.array(data, dtype='float32')\n",
    "data = data[:,:2]\n",
    "\n",
    "target = np.array(target,  dtype='float32').reshape(506, 1)\n",
    "\n",
    "data.shape, features.shape, target.shape, labels.shape, type(data), type(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_array(data_arrays, batch_size, is_train=True): #@save\n",
    "    \"\"\"Construct a Gluon data iterator.\"\"\"\n",
    "    dataset = gluon.data.ArrayDataset(*data_arrays)\n",
    "    return gluon.data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "\n",
    "batch_size = 10\n",
    "data_iter = load_array((features, labels), batch_size)\n",
    "data_iter_1 = load_array((data, target), batch_size)\n",
    "# print(next(iter(data_iter_1[0:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon import nn\n",
    "from mxnet import init\n",
    "from mxnet.gluon import Trainer\n",
    "\n",
    "# set layer and model type\n",
    "net_ = nn.Sequential()\n",
    "net_.add(nn.Dense(1))\n",
    "# params init\n",
    "net_.initialize(init=init.Normal(sigma=0.01), verbose=False)\n",
    "# loss squared\n",
    "loss = gluon.loss.L1Loss()\n",
    "# SGD\n",
    "trainer = Trainer(net_.collect_params(), 'sgd', {'learning_rate': 0.03})\n",
    "print(net_.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 3\n",
    "\n",
    "for e in range(epoch):\n",
    "    for X, y in data_iter:\n",
    "        with autograd.record():\n",
    "            l_ = loss(net_(X), y)\n",
    "        l_.backward()\n",
    "        trainer.step(batch_size)\n",
    "    l = loss(net_(features), labels)\n",
    "    print(f'total loss per epoch {e} is {l.mean().asnumpy():f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net_[0].weight.data(), net_[0].bias.data())\n",
    "print(true_w, true_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.5 The Image Classification Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from d2l import mxnet as d2l\n",
    "from mxnet import gluon, autograd, np, npx\n",
    "import sys\n",
    "from IPython import display\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = gluon.data.vision.FashionMNIST(train=True)\n",
    "mnist_test = gluon.data.vision.FashionMNIST(train=False)\n",
    "print(len(mnist_train), len(mnist_test))\n",
    "print(mnist_train[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the iterator\n",
    "batch_size = 256\n",
    "\n",
    "transformation = gluon.data.vision.transforms.ToTensor()\n",
    "                      \n",
    "train_iterator = gluon.data.DataLoader(\n",
    "    mnist_train.transform_first(transformation),\n",
    "    batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=-1)\n",
    "\n",
    "test_iterator = gluon.data.DataLoader(\n",
    "    mnist_test.transform_first(transformation),\n",
    "    batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fashion_mnist_labels(labels):\n",
    "    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat', \\\n",
    "                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "    return [text_labels[int(i)] for i in labels]\n",
    "\n",
    "def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):\n",
    "    figsize = (num_cols * scale, num_rows * scale)\n",
    "    _, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (ax, img) in enumerate(zip(axes, imgs)):\n",
    "        ax.imshow(d2l.numpy(img))\n",
    "        ax.axes.get_xaxis().set_visible(False)\n",
    "        ax.axes.get_yaxis().set_visible(False)\n",
    "        if titles:\n",
    "            ax.set_title(titles[i])\n",
    "    return \n",
    "\n",
    "X, y = mnist_train[:18]\n",
    "\n",
    "show_images(X.squeeze(axis=-1), 3, 6, titles=(get_fashion_mnist_labels(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    X_ = np.exp(X) \n",
    "    X_sum = X_.sum(1, keepdims=True)\n",
    "    return X_ / X_sum\n",
    "\n",
    "def net(X):\n",
    "    return softmax(np.dot(X.reshape((-1, W.shape[0])), W) + b)\n",
    "\n",
    "def cross_entropy(y_hat, y):\n",
    "    return - np.log(y_hat[range(len(y_hat)), y])\n",
    "\n",
    "def accuracy(y_hat, y):\n",
    "    \"\"\"Compute the number of correct predictions.\"\"\"\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = y_hat.argmax(axis=1)\n",
    "    cmp = y_hat.astype(y.dtype) == y\n",
    "    return float(d2l.reduce_sum(cmp.astype(y.dtype)))\n",
    "\n",
    "class Accumulator:\n",
    "    \"\"\"For accumulating sums over `n` variables.\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "def evaluate_accuracy(net, data_iter):\n",
    "    \"\"\"Compute the accuracy for a model on a dataset.\"\"\"\n",
    "    metric = Accumulator(2) # No. of correct predictions, no. of predictions\n",
    "    for _, (X, y) in enumerate(data_iter):\n",
    "        metric.add(accuracy(net(X), y), y.size)\n",
    "    return metric[0] / metric[1]\n",
    "\n",
    "def train_epoch_ch3(net, train_iter, loss, updater):\n",
    "    # Sum of training loss, sum of training accuracy, no. of examples\n",
    "    metric = Accumulator(3)\n",
    "    if isinstance(updater, gluon.Trainer):\n",
    "        updater = updater.step\n",
    "    for X, y in train_iter:\n",
    "    # Compute gradients and update parameters\n",
    "        with autograd.record():\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "        l.backward()\n",
    "        updater(X.shape[0])\n",
    "        metric.add(float(l.sum()), accuracy(y_hat, y), y.size)\n",
    "    # Return training loss and training accuracy\n",
    "#     print(f'accuracy per epoch is {metric[0] / metric[2], metric[1] / metric[2]}')\n",
    "    return metric[0] / metric[2], metric[1] / metric[2]\n",
    "\n",
    "class Animator:\n",
    "    \"\"\"For plotting data in animation.\"\"\"\n",
    "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
    "                    ylim=None, xscale='linear', yscale='linear',\n",
    "                    fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1, figsize=(5, 5)):\n",
    "        # Incrementally plot multiple lines\n",
    "        if legend is None:\n",
    "            legend = []\n",
    "        d2l.use_svg_display()\n",
    "        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)\n",
    "        if nrows * ncols == 1:\n",
    "            self.axes = [self.axes, ]\n",
    "        # Use a lambda function to capture arguments\n",
    "        self.config_axes = lambda: d2l.set_axes(self.axes[0], xlabel, \\\n",
    "                                                ylabel, xlim, ylim, xscale, yscale, legend)\n",
    "        self.X, self.Y, self.fmts = None, None, fmts\n",
    "    \n",
    "    def add(self, x, y):\n",
    "        # Add multiple data points into the figure\n",
    "        if not hasattr(y, \"__len__\"):\n",
    "            y = [y]\n",
    "        n = len(y)\n",
    "        if not hasattr(x, \"__len__\"):\n",
    "            x = [x] * n\n",
    "        if not self.X:\n",
    "            self.X = [[] for _ in range(n)]\n",
    "        if not self.Y:\n",
    "            self.Y = [[] for _ in range(n)]\n",
    "        for i, (a, b) in enumerate(zip(x, y)):\n",
    "            if a is not None and b is not None:\n",
    "                self.X[i].append(a)\n",
    "                self.Y[i].append(b)\n",
    "        self.axes[0].cla()\n",
    "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "            self.axes[0].plot(x, y, fmt)\n",
    "        self.config_axes()\n",
    "        display.display(self.fig)\n",
    "        display.clear_output(wait=True)\n",
    "        \n",
    "def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):\n",
    "    \"\"\"Train a model (defined in Chapter 3).\"\"\"\n",
    "    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9], \\\n",
    "                            legend=['train loss', 'train acc', 'test acc'])\n",
    "    for epoch in range(num_epochs):\n",
    "        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)\n",
    "        test_acc = evaluate_accuracy(net, test_iter)\n",
    "#         print(f'test acc {test_acc}')\n",
    "        animator.add(epoch + 1, train_metrics + (test_acc,))\n",
    "        \n",
    "    train_loss, train_acc = train_metrics\n",
    "    \n",
    "    assert train_loss < 0.5, train_loss\n",
    "    assert train_acc <= 1 and train_acc > 0.7, train_acc\n",
    "    assert test_acc <= 1 and test_acc > 0.7, test_acc\n",
    "    \n",
    "def updater(batch_size):\n",
    "    return d2l.sgd([W, b], lr, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "num_epochs = 10\n",
    "# flattern the image 28 * 28 = become 784 vector, add number of \n",
    "#classes10 marix is 784 x 10 + bias 1x10 row vertor\\\n",
    "W = np.random.normal(loc=0, scale=0.01, size=(784, 10))\n",
    "b = np.zeros(10)\n",
    "\n",
    "W.attach_grad()\n",
    "b.attach_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ch3(net, train_iterator, test_iterator, cross_entropy, num_epochs, updater)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_train_ch3(net, test, number_items=6):\n",
    "    for X, y in test:\n",
    "        break\n",
    "    true = get_fashion_mnist_labels(y)\n",
    "    pred = get_fashion_mnist_labels(np.argmax(net(X), axis=1))\n",
    "    predicted_labels = get_fashion_mnist_labels(y)\n",
    "    titles = [true +'\\n' + pred for true, pred in zip(true, predicted_labels)]\n",
    "    show_images(np.reshape(X[0:number_items], (number_items, 28, 28)), 1, \\\n",
    "                number_items, titles=titles[0:number_items])\n",
    "    \n",
    "predict_train_ch3(net, test_iterator, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.7 Concise Implementation of Softmax Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import mxnet as d2l\n",
    "from mxnet import gluon, init, npx\n",
    "from mxnet.gluon import nn\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_ = nn.Sequential()\n",
    "net_.add(nn.Dense(10))\n",
    "net_.initialize(init.Normal(sigma=0.01))\n",
    "net_.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "traner = gluon.Trainer(net_.collect_params(), 'sgd', {'learning_rate': 0.1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eph = 10\n",
    "d2l.train_ch3(net_, train_iter, test_iter, loss, eph, traner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_train_ch3_(net, test, number_items=6):\n",
    "    for X, y in test:\n",
    "        break\n",
    "    true = get_fashion_mnist_labels(y)\n",
    "    pred = get_fashion_mnist_labels(np.argmax(net(X), axis=1))\n",
    "    predicted_labels = get_fashion_mnist_labels(y)\n",
    "    titles = [true +'\\n' + pred for true, pred in zip(true, predicted_labels)]\n",
    "    show_images(np.reshape(X[0:number_items], (number_items, 28, 28)), 1, \\\n",
    "                number_items, titles=titles[0:number_items])\n",
    "# test it \n",
    "predict_train_ch3_(net_, test_iter, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP chapter\n",
    "%matplotlib inline\n",
    "from d2l import mxnet as d2l\n",
    "from mxnet import autograd, np, npx\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-8, 8, 0.1)\n",
    "x.attach_grad()\n",
    "\n",
    "with autograd.record():\n",
    "    y = npx.sigmoid(x) # Sigmoid function \n",
    "d2l.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward() # derivation \n",
    "d2l.plot(x, x.grad, 'x', 'grad of sigmoid', figsize=(5, 2.5))\n",
    "         # ploy gradients / then input 0 than max sigmoid is 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with autograd.record():\n",
    "    yy = np.tanh(x)\n",
    "d2l.plot(x, yy, 'x', 'tanh fucntion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy.backward()\n",
    "d2l.plot(x, x.grad, 'x', 'gradients of tanh fucntion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP from scratch \n",
    "from d2l import mxnet as d2l\n",
    "from mxnet import gluon, np, npx\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = d2l.load_data_fashion_mnist(batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 28 x 28 picture = 784 input for 10 classes\n",
    "# 1 H layer and  256 units? \n",
    "# layer to power of 2 ## for every layer we keep W + b\n",
    "\n",
    "def run(units, lr):\n",
    "    num_inputs, num_outputs, num_hiddens = 784, 10, units\n",
    "    W1 = np.random.normal(scale=0.01, size=(num_inputs, num_hiddens))\n",
    "    b1 = np.zeros(num_hiddens)\n",
    "    W2 = np.random.normal(scale=0.01, size=(num_hiddens, num_hiddens))\n",
    "    b2 = np.zeros(num_hiddens)\n",
    "#     W3 = np.random.normal(scale=0.01, size=(num_hiddens, num_outputs))\n",
    "#     b3 = np.zeros(num_outputs)\n",
    "    params = [W1, b1, W2, b2]\n",
    "\n",
    "    for param in params:\n",
    "        param.attach_grad()\n",
    "\n",
    "    def mlp(X):\n",
    "        X = X.reshape((-1, num_inputs))\n",
    "        H = relu(np.dot(X, W1) + b1)\n",
    "        return np.dot(H, W2) + b2\n",
    "\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "    def relu(X):\n",
    "        return np.maximum(X, 0)\n",
    "\n",
    "    num_epochs = 50\n",
    "\n",
    "    train_acc = d2l.train_ch3(mlp, train, test, loss, num_epochs, \\\n",
    "                  lambda batch_size: d2l.sgd(params, lr, batch_size))\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for lr in [0.5, 0.9, 1, 2]:\n",
    "    l = run(1024, lr)\n",
    "    res.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip([round(i, 3) for i in res], [0.5, 0.9, 1, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2 layers\n",
    "(0.864, 512),  (0.867, 1024)\n",
    "3 layers \n",
    "[(0.858, 512), (0.866, 1024)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import mxnet as d2l\n",
    "from mxnet import gluon, init, npx\n",
    "from mxnet.gluon import nn\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net__ = nn.Sequential()\n",
    "net__.add(nn.Dense(256, activation='relu'), nn.Dense(10))\n",
    "net__.initialize(init.Normal(sigma=0.01))\n",
    "net__.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net__.collect_params(), 'sgd', {'learning_rate': 0.5})\n",
    "train, test = d2l.load_data_fashion_mnist(batch_size=64)\n",
    "\n",
    "d2l.train_ch3(net__, train, test, loss, num_epochs=30, updater=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polinimial regression \n",
    "from d2l import mxnet as d2l\n",
    "from mxnet import gluon, np, npx\n",
    "from mxnet.gluon import nn\n",
    "import math\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_degree = 40 # Maximum degree of the polynomial\n",
    "n_train, n_test = 100, 100 # Training and test dataset sizes\n",
    "true_w = np.zeros(max_degree) # Allocate lots of empty space\n",
    "true_w[0:4] = np.array([5, 1.2, -3.4, 5.6])\n",
    "features = np.random.normal(size=(n_train + n_test, 1))\n",
    "np.random.shuffle(features)\n",
    "poly_features = np.power(features, np.arange(max_degree).reshape(1, -1))\n",
    "for i in range(max_degree):\n",
    "    poly_features[:, i] /= math.gamma(i + 1) # `gamma(n)` = (n-1)!\n",
    "# Shape of `labels`: (`n_train` + `n_test`,)\n",
    "labels = np.dot(poly_features, true_w)\n",
    "labels += np.random.normal(scale=0.1, size=labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_loss(net, data_iter, loss): #@save\n",
    "    \"\"\"Evaluate the loss of a model on the given dataset.\"\"\"\n",
    "    metric = d2l.Accumulator(2) # Sum of losses, no. of examples\n",
    "    for X, y in data_iter:\n",
    "        l = loss(net(X), y)\n",
    "        metric.add(d2l.reduce_sum(l), l.size)\n",
    "    return metric[0] / metric[1]\n",
    "\n",
    "import numpy\n",
    "\n",
    "def train(n_train=100, n_test=100, num_epochs=400, max_degree=40):\n",
    "     # Training and test dataset sizes\n",
    "    true_w = np.zeros(max_degree) # Allocate lots of empty space\n",
    "    true_w[0:4] = np.array([5, 1.2, -3.4, 5.6])\n",
    "    features = np.random.normal(size=(n_train + n_test, 1))\n",
    "    np.random.shuffle(features)\n",
    "    poly_features = np.power(features, np.arange(max_degree).reshape(1, -1))\n",
    "    #\n",
    "    mu = numpy.mean([i for i in range(max_degree)])\n",
    "    s = numpy.std([i for i in range(max_degree)])\n",
    "    \n",
    "    for i in range(max_degree):\n",
    "    #   poly_features[:, i] /= math.gamma(i + 1) # `gamma(n)` = (n-1)!\n",
    "        poly_features[:, i] /= ((i - mu) / s) + 2 # from the task -> Normalization\n",
    "    \n",
    "    # Shape of `labels`: (`n_train` + `n_test`,)\n",
    "    labels = np.dot(poly_features, true_w)\n",
    "    labels += np.random.normal(scale=0.1, size=labels.shape)\n",
    "    # data\n",
    "    train_features, test_features = poly_features[:n_train, :4], poly_features[n_train:, :4]\n",
    "    train_labels, test_labels = labels[:n_train], labels[n_train:]\n",
    "    # loss and net\n",
    "    loss = gluon.loss.L2Loss()\n",
    "    net = nn.Sequential()\n",
    "    # Switch off the bias since we already catered for it in the polynomial\n",
    "    # features\n",
    "    net.add(nn.Dense(1, use_bias=False))\n",
    "    net.initialize()\n",
    "    batch_size = min(10, train_labels.shape[0])\n",
    "    # iter \n",
    "    train_iter = d2l.load_array((train_features, train_labels), batch_size, is_train=True)\n",
    "    test_iter = d2l.load_array((test_features, test_labels), batch_size, is_train=False)\n",
    "    # SGD, draw\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.01})\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss', yscale='log', \\\n",
    "                            xlim=[1, num_epochs], ylim=[1e-3, 1e2], legend=['train', 'test'])\n",
    "    # iterate \n",
    "    losss = []\n",
    "    for epoch in range(num_epochs):\n",
    "        d2l.train_epoch_ch3(net, train_iter, loss, trainer)\n",
    "        if epoch + 1 == num_epochs:\n",
    "            print(f'train loss is {evaluate_loss(net, train_iter, loss)}, \\\n",
    "                            test lost {evaluate_loss(net, test_iter, loss)}, \\\n",
    "                            with degree {max_degree}')\n",
    "            losss.append(evaluate_loss(net, train_iter, loss))\n",
    "            \n",
    "        if epoch == 0 or (epoch + 1) % 20 == 0:\n",
    "            animator.add(epoch + 1, (evaluate_loss(net, train_iter, loss), \\\n",
    "                                     evaluate_loss(net, test_iter, loss)))\n",
    "            \n",
    "    print('weight:', net[0].weight.data().asnumpy())\n",
    "#     return losss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for number_samples in [100, 500]: # test number of sample\n",
    "    train(n_train=number_samples, n_test=number_samples, num_epochs=400, max_degree=40) # for best degree\n",
    "    # ([5, 1.2, -3.4, 5.6]) # true coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losss = {}\n",
    "\n",
    "for degree in range(5, 60, 5):\n",
    "    ls = train(poly_features[:n_train, :4], poly_features[n_train:, :4], \\\n",
    "                                  labels[:n_train], labels[n_train:], max_degree=degree)\n",
    "    losss[ls[0]] = degree    \n",
    "# np.array([5, 1.2, -3.4, 5.6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(list(losss.values()), list(losss.keys()))\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight decay (l1, L2 norm)\n",
    "%matplotlib inline\n",
    "from d2l import mxnet as d2l\n",
    "from mxnet import autograd, gluon, init, np, npx\n",
    "from mxnet.gluon import nn\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train, num_test, num_inputs, batch_size = 50, 100, 300, 20\n",
    "\n",
    "true_w, true_b = np.ones((num_inputs, 1)) * 0.01, 0.05 # add noise\n",
    "\n",
    "train_data = d2l.synthetic_data(true_w, true_b, num_train)\n",
    "test_data = d2l.synthetic_data(true_w, true_b, num_test)\n",
    "\n",
    "train_iter = d2l.load_array(train_data, batch_size, is_train=True)\n",
    "test_iter = d2l.load_array(test_data, batch_size, is_train=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_weight():\n",
    "    w = np.random.normal(loc=1, scale=0.01, size=(num_inputs, 1))\n",
    "    b = np.zeros(1)\n",
    "    w.attach_grad()\n",
    "    b.attach_grad()\n",
    "    return [w, b]\n",
    "\n",
    "def ridge_l2(w):\n",
    "    return (w**2).sum() / 2\n",
    "\n",
    "def lasso_l1(w):\n",
    "    return (np.abs(w)).sum() / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_(lambda_):\n",
    "    w, b = set_weight()\n",
    "    net_, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss\n",
    "    lr, epoch = 0.003, 100\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss', yscale='log', xlim=[5, epoch], \\\n",
    "                            legend=['train', 'test'])\n",
    "    for e in range(epoch):\n",
    "        for X, y in train_iter:\n",
    "            with autograd.record():\n",
    "                model_result = net_(X)\n",
    "                model_loss = loss(model_result, y) + lambda_ * lasso_l1(w)\n",
    "            model_loss.backward()\n",
    "            d2l.sgd([w, b], lr, batch_size)\n",
    "        if (e + 1) % 5 == 0:\n",
    "            animator.add(e + 1, \\\n",
    "                         (d2l.evaluate_loss(net_, train_iter, loss), \\\n",
    "                         (d2l.evaluate_loss(net_, test_iter, loss))))\n",
    "                         \n",
    "    print('L2 norm of w:', np.linalg.norm(w))\n",
    "                \n",
    "train_(12)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_(12)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 10_000\n",
    "tr_w, tr_b = np.ones((SIZE, 1)) * 0.3, 0.2\n",
    "all_data = d2l.synthetic_data(tr_w, tr_b, SIZE)\n",
    "# valid / train size\n",
    "valid_size = int(SIZE - SIZE * 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = all_data[0][:valid_size],  all_data[0][valid_size:], \\\n",
    "                                            all_data[1][:valid_size], all_data[1][valid_size:]\n",
    "x_train.shape, x_valid.shape, y_train.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = int(x_train.shape[0] * 0.8)\n",
    "\n",
    "train, y_train, test, y_test = x_train[:test_size], y_train[:test_size],\\\n",
    "                                    x_train[test_size:], y_train[test_size:]\n",
    "train.shape, y_train.shape, test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_s = 200\n",
    "\n",
    "iterator_train = d2l.load_array((train, y_train), batch_s, is_train=True)\n",
    "iterator_test = d2l.load_array((test, y_test), batch_s, is_train=False)\n",
    "iterator_valid = d2l.load_array((x_valid, y_valid), batch_s, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_concise(iterator_train, iterator_test, iterator_valid, wd):\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(1))\n",
    "    net.initialize(init.Normal(sigma=1))\n",
    "    loss = gluon.loss.L2Loss()\n",
    "    num_epochs, lr = 100, 0.003\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd',  {'learning_rate': lr, 'wd': wd})\n",
    "    net.collect_params('.*bias').setattr('wd_mult', 0)\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    valid_loss = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for X, y in iterator_train:\n",
    "            with autograd.record():\n",
    "                l = loss(net(X), y)\n",
    "        l.backward()\n",
    "        trainer.step(batch_s)\n",
    "        train_loss.append(d2l.evaluate_loss(net, iterator_train, loss))\n",
    "        test_loss.append(d2l.evaluate_loss(net, iterator_test, loss))\n",
    "        valid_loss.append(d2l.evaluate_loss(net, iterator_valid, loss))\n",
    "    plt.plot([i for i in range(num_epochs)], [i for i in train_loss], label='train')\n",
    "    plt.plot([i for i in range(num_epochs)], [i for i in test_loss], label='test')\n",
    "    plt.plot([i for i in range(num_epochs)], [i for i in valid_loss], label='valid')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.title(f'train, test, valid loss changing by epochs with weight decay {wd} value')\n",
    "    plt.legend()\n",
    "    plt.show();\n",
    "\n",
    "train_concise(iterator_train, iterator_test, iterator_valid, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_concise(iterator_train, iterator_test, iterator_valid, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_concise(iterator_train, iterator_test, iterator_valid, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DropOut\n",
    "from d2l import mxnet as d2l\n",
    "from mxnet import autograd, gluon, init, np, npx\n",
    "from mxnet.gluon import nn\n",
    "npx.set_np()\n",
    "\n",
    "def DropOut(X, p):\n",
    "    if p == 1:\n",
    "        return np.zeros_like(X)\n",
    "    if p == 0:\n",
    "        return X\n",
    "    mask = np.random.uniform(0, 1, X.shape) > p\n",
    "    return mask.astype(np.float32) * X / (1.0 - p)\n",
    "\n",
    "def ridge_l2(w):\n",
    "    return (w**2).sum() / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fashion Dataset\n",
    "inputs, outputs, layer1, layer2 = 784, 10, 256, 256\n",
    "\n",
    "W1 = np.random.normal(loc=0, scale=0.1, size=(inputs, layer1))\n",
    "B1 = np.zeros(layer1)\n",
    "\n",
    "W2 = np.random.normal(loc=0, scale=0.1, size=(layer1, layer2))\n",
    "B2 = np.zeros(layer2)\n",
    "\n",
    "W3 = np.random.normal(loc=0, scale=0.1, size=(layer2, outputs))\n",
    "B3 = np.zeros(outputs)\n",
    "\n",
    "params = [W1, B1, W2, B2, W3, B3]\n",
    "\n",
    "for p in params:\n",
    "    p.attach_grad()\n",
    "    \n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "    \n",
    "epoch = 2\n",
    "dropout1, dropout2 = 0.5, 0.6\n",
    "l2_coeff = 10\n",
    "lr = 0.003\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(train_iter, test_iter, epoch, l2_coeff, dropout1, dropout2, lr, batch_size):\n",
    "    #\n",
    "    def updater(batch_size):\n",
    "        return d2l.sgd([W, b], lr, batch_size)\n",
    "    \n",
    "    def accuracy(y_hat, y):\n",
    "        if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "            y_hat = d2l.argmax(y_hat, axis=1)\n",
    "        cmp = d2l.astype(y_hat, y.dtype) == y\n",
    "        return float(d2l.reduce_sum(d2l.astype(cmp, y.dtype)))\n",
    "    #\n",
    "    def evaluate_accuracy(net, data_iter):\n",
    "        metric = d2l.Accumulator(2)\n",
    "        for X, y in data_iter:\n",
    "            metric.add(accuracy(net(X), y), d2l.size(y))\n",
    "        return metric[0] / metric[1]\n",
    "    #\n",
    "    def net_(X):\n",
    "        X = X.reshape(-1, inputs)\n",
    "        H1 = npx.relu(np.dot(X, W1) + B1)        \n",
    "        if autograd.is_training():\n",
    "            H1 = DropOut(H1, dropout1)\n",
    "        H2 = npx.relu(np.dot(H1, W2) + B2)\n",
    "        if autograd.is_training():\n",
    "            H2 = DropOut(H2, dropout2)\n",
    "        return np.dot(H2, W3) + B3\n",
    "    \n",
    "    def train_epoch_ch3(net, train_iter, loss, updater):\n",
    "        metric = d2l.Accumulator(3)\n",
    "        if isinstance(updater, gluon.Trainer):\n",
    "            updater = updater.step\n",
    "        for X, y in train_iter:\n",
    "            with autograd.record():\n",
    "                y_hat = net(X)\n",
    "                l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            updater(X.shape[0])\n",
    "            metric.add(float(l.sum()), accuracy(y_hat, y), y.size)\n",
    "        return metric[0] / metric[2], metric[1] / metric[2]\n",
    "    #\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    updater = gluon.Trainer\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss', yscale='log', \\\n",
    "                            xlim=[5, epoch], legend=['train', 'test'])\n",
    "    #\n",
    "    for epoch in range(epoch):\n",
    "        train_metrics = train_epoch_ch3(net_, train_iter, loss, updater)\n",
    "        test_acc = evaluate_accuracy(net, test_iter)\n",
    "        animator.add(epoch + 1, train_metrics + (test_acc,))\n",
    "        \n",
    "    train_loss, train_acc = train_metrics\n",
    "    print(train_loss, train_acc)\n",
    "\n",
    "train_net(train_iter, test_iter, epoch, l2_coeff, dropout1, dropout2, lr, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout1, dropout2 = 0.5, 0.6\n",
    "lambda_ = 10\n",
    "\n",
    "def net(X, lambda_):\n",
    "    X = X.reshape(-1, inputs)\n",
    "    H1 = npx.relu(np.dot(X, W1) + B1)        \n",
    "    if autograd.is_training():\n",
    "        H1 = DropOut(H1, dropout1)\n",
    "    H2 = npx.relu(np.dot(H1, W2) + B2)\n",
    "    if autograd.is_training():\n",
    "        H2 = DropOut(H2, dropout2)\n",
    "    return np.dot(H2, W3) + B3\n",
    "\n",
    "num_epochs, lr, batch_size = 2, 0.5, 256\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, \\\n",
    "                                            lambda batch_size: d2l.sgd(params, lr, batch_size)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 1\n",
    "dropout_list = list(zip([i / 10 for i in range(1, 10)], [i / 10 for i in range(10, 1, -1)]))\n",
    "from mxnet import gluon\n",
    "from d2l import mxnet as d2l\n",
    "from mxnet import autograd, init, np, npx\n",
    "npx.set_np()\n",
    "\n",
    "def DropOut(X, p):\n",
    "    if p == 1:\n",
    "        return np.zeros_like(X)\n",
    "    if p == 0:\n",
    "        return X\n",
    "    mask = np.random.uniform(0, 1, X.shape) > p\n",
    "    return mask.astype(np.float32) * X / (1.0 - p)\n",
    "\n",
    "num_epochs, lr, batch_size = 10, 0.5, 256\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_dropout(dropout1, dropout2):\n",
    "    inputs, outputs, layer1, layer2 = 784, 10, 256, 256\n",
    "    W1 = np.random.normal(loc=0, scale=0.1, size=(inputs, layer1))\n",
    "    B1 = np.zeros(layer1)\n",
    "    W2 = np.random.normal(loc=0, scale=0.1, size=(layer1, layer2))\n",
    "    B2 = np.zeros(layer2)\n",
    "    W3 = np.random.normal(loc=0, scale=0.1, size=(layer2, outputs))\n",
    "    B3 = np.zeros(outputs)\n",
    "    params = [W1, B1, W2, B2, W3, B3]\n",
    "    for p in params:\n",
    "        p.attach_grad()\n",
    "        \n",
    "    def net__(X):\n",
    "        X = X.reshape(-1, inputs)\n",
    "        H1 = npx.relu(np.dot(X, W1) + B1)\n",
    "        if autograd.is_training():\n",
    "            H1 = DropOut(H1, dropout1)\n",
    "        H2 = npx.relu(np.dot(H1, W2) + B2)\n",
    "        if autograd.is_training():\n",
    "            H2 = DropOut(H2, dropout2)\n",
    "        return np.dot(H2, W3) + B3\n",
    "\n",
    "    train_loss, train_acc, test_acc = d2l.train_ch3(net__, train_iter, test_iter, loss, num_epochs, \\\n",
    "                                            lambda batch_size: d2l.sgd(params, lr, batch_size))\n",
    "    \n",
    "    return train_loss, train_acc, test_acc, dropout1, dropout2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_dropout(0.5, 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for dropout1, dropout2 in dropout_list:\n",
    "    train_loss, train_acc, test_acc, dropout1, dropout2 = play_dropout(dropout1, dropout2)\n",
    "    print(train_loss, train_acc, test_acc, dropout1, dropout2)\n",
    "    dd_ = [train_loss, train_acc, test_acc, dropout1, dropout2]\n",
    "    \n",
    "    data.append(dd_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 5 - 6 \n",
    "from mxnet import gluon\n",
    "from d2l import mxnet as d2l\n",
    "from mxnet import autograd, init, np, npx\n",
    "npx.set_np()\n",
    "\n",
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DropOut_(X, p):\n",
    "    if p == 1:\n",
    "        return np.zeros_like(X)\n",
    "    if p == 0:\n",
    "        return X\n",
    "    mask = np.random.uniform(0, 1, X.shape) > p\n",
    "    return mask.astype(np.float32) * X / (1.0 - p)\n",
    "\n",
    "def l2_penalty(w):\n",
    "    return (w**2).sum() / 2\n",
    "\n",
    "def net_(X):\n",
    "    X = X.reshape(-1, num_inputs)\n",
    "    H1 = npx.relu(np.dot(X, W1) + b1)\n",
    "    if autograd.is_training():\n",
    "        H1 = DropOut_(H1, dropout1)\n",
    "    H2 = npx.relu(np.dot(H1, W2) + b2)\n",
    "    if autograd.is_training():\n",
    "        H2 = DropOut_(H2, dropout2)\n",
    "    return np.dot(H2, W3) + b3\n",
    "\n",
    "def evaluate_accuracy_(net, data_iter):\n",
    "    \"\"\"Compute the accuracy for a model on a dataset.\"\"\"\n",
    "    metric = d2l.Accumulator(2)  # No. of correct predictions, no. of predictions\n",
    "    for X, y in data_iter:\n",
    "        metric.add(accuracy_(net(X), y), d2l.size(y))\n",
    "    return metric[0] / metric[1]\n",
    "\n",
    "def accuracy_(y_hat, y):\n",
    "    \"\"\"Compute the number of correct predictions.\"\"\"\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = d2l.argmax(y_hat, axis=1)\n",
    "    cmp = d2l.astype(y_hat, y.dtype) == y\n",
    "    return float(d2l.reduce_sum(d2l.astype(cmp, y.dtype)))\n",
    "\n",
    "def train_epoch_ch3_(net, train_iter, loss, updater):\n",
    "    # Sum of training loss, sum of training accuracy, no. of examples\n",
    "    metric = d2l.Accumulator(3)\n",
    "    if isinstance(updater, gluon.Trainer):\n",
    "        updater = updater.step\n",
    "    for X, y in train_iter:\n",
    "        with autograd.record():\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y) + l2_coeff * l2_penalty(W3)\n",
    "        l.backward()\n",
    "        updater(X.shape[0])\n",
    "        metric.add(float(l.sum()), accuracy_(y_hat, y), y.size)\n",
    "    # Return training loss and training accuracy \n",
    "    print('L2 norm of w:', np.linalg.norm(W3))\n",
    "    return metric[0] / metric[2], metric[1] / metric[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256\n",
    "W1 = np.random.normal(scale=0.01, size=(num_inputs, num_hiddens1))\n",
    "b1 = np.zeros(num_hiddens1)\n",
    "W2 = np.random.normal(scale=0.01, size=(num_hiddens1, num_hiddens2))\n",
    "b2 = np.zeros(num_hiddens2)\n",
    "W3 = np.random.normal(scale=0.01, size=(num_hiddens2, num_outputs))\n",
    "b3 = np.zeros(num_outputs)\n",
    "params = [W1, b1, W2, b2, W3, b3]\n",
    "\n",
    "for param in params:\n",
    "    param.attach_grad()\n",
    "#\n",
    "num_epochs = 10\n",
    "dropout1, dropout2 = 0.5, 0.6\n",
    "l2_coeff = 0\n",
    "lr = 0.1\n",
    "#\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "updater = lambda batch_size: d2l.sgd(params, lr, batch_size)\n",
    "#\n",
    "def train_ch3_(net, train_iter, test_iter, loss, num_epochs, updater):\n",
    "    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],\n",
    "                            legend=['train loss', 'train acc', 'test acc'])\n",
    "    for epoch in range(num_epochs):\n",
    "        train_metrics = train_epoch_ch3_(net, train_iter, loss, updater)\n",
    "        test_acc = evaluate_accuracy_(net, test_iter)\n",
    "        animator.add(epoch + 1, train_metrics + (test_acc,))\n",
    "    train_loss, train_acc = train_metrics\n",
    "    print(f'test acc {test_acc}')\n",
    "    print(f' loss is {train_loss}, accuracy is {train_acc}')\n",
    "#     assert train_loss < 0.5, train_loss\n",
    "#     assert train_acc <= 1 and train_acc > 0.7, train_acc\n",
    "#     assert test_acc <= 1 and test_acc > 0.7, test_acc\n",
    "\n",
    "train_ch3_(net_, train_iter, test_iter, loss, num_epochs, updater)\n",
    "# test acc 0.7335 -> l2_coeff = 0.50   -> dropout1, dropout2 = 0, 0\n",
    "# test acc 0.7904 -> l2_coeff = 0.10   -> dropout1, dropout2 = 0, 0\n",
    "# test acc 0.8298 -> l2_coeff = 0.01   -> dropout1, dropout2 = 0, 0\n",
    "# test acc 0.8162 -> l2_coeff = 0.01   -> dropout1, dropout2 = 0.4, 0.5\n",
    "# test acc 0.8145 -> l2_coeff = 0.00   -> dropout1, dropout2 = 0.4, 0.5\n",
    "# test acc 0.8487 -> l2_coeff = 0.00   -> dropout1, dropout2 = 0.4, 0.6\n",
    "# test acc 0.8602 -> l2_coeff = 0.00   -> dropout1, dropout2 = 0.5, 0.6\n",
    "# test acc 0.8057 -> l2_coeff = 0.10   -> dropout1, dropout2 = 0.5, 0.6\n",
    "# test acc 0.8374 -> l2_coeff = 0.01   -> dropout1, dropout2 = 0.5, 0.6\n",
    "# test acc 0.8598 -> l2_coeff = 0.001  -> dropout1, dropout2 = 0.5, 0.6\n",
    "# test acc 0.8570 -> l2_coeff = 0.0001 -> dropout1, dropout2 = 0.5, 0.6\n",
    "\n",
    "# for 5th tasl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical stability chapter\n",
    "\n",
    "from mxnet import gluon\n",
    "from d2l import mxnet as d2l\n",
    "from mxnet import autograd, init, np, npx\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.arange(-10, 10, 0.1, dtype=float)\n",
    "\n",
    "data.attach_grad()\n",
    "\n",
    "with autograd.record():\n",
    "    y = npx.sigmoid(data)\n",
    "    \n",
    "y.backward()\n",
    "\n",
    "d2l.plot(data, [y, data.grad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the book dowload fucntions\n",
    "\n",
    "def download(name, cache_dir=os.path.join('..', 'data')): #@save\n",
    "\"\"\"Download a file inserted into DATA_HUB, return the local filename.\"\"\"\n",
    "    assert name in DATA_HUB, f\"{name} does not exist in {DATA_HUB}.\"\n",
    "    url, sha1_hash = DATA_HUB[name]\n",
    "    d2l.mkdir_if_not_exist(cache_dir)\n",
    "    fname = os.path.join(cache_dir, url.split('/')[-1])\n",
    "    if os.path.exists(fname):\n",
    "        sha1 = hashlib.sha1()\n",
    "        with open(fname, 'rb') as f:\n",
    "            while True:\n",
    "                data = f.read(1048576)\n",
    "                if not data:\n",
    "                    break\n",
    "    sha1.update(data)\n",
    "    if sha1.hexdigest() == sha1_hash:\n",
    "    return fname # Hit cache\n",
    "    print(f'Downloading {fname} from {url}...')\n",
    "    r = requests.get(url, stream=True, verify=True)\n",
    "    with open(fname, 'wb') as f:\n",
    "    f.write(r.content)\n",
    "    return fname\n",
    "\n",
    "def download_extract(name, folder=None): #@save\n",
    "    \"\"\"Download and extract a zip/tar file.\"\"\"\n",
    "    fname = download(name)\n",
    "    base_dir = os.path.dirname(fname)\n",
    "    data_dir, ext = os.path.splitext(fname)\n",
    "    if ext == '.zip':\n",
    "    fp = zipfile.ZipFile(fname, 'r')\n",
    "    elif ext in ('.tar', '.gz'):\n",
    "    fp = tarfile.open(fname, 'r')\n",
    "    else:\n",
    "    assert False, 'Only zip/tar files can be extracted.'\n",
    "    fp.extractall(base_dir)\n",
    "    return os.path.join(base_dir, folder) if folder else data_dir\n",
    "def download_all(): #@save\n",
    "    \"\"\"Download all files in the DATA_HUB.\"\"\"\n",
    "    for name in DATA_HUB:\n",
    "    download(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling\n",
    "numeric_features = all_features.dtypes[all_features.dtypes != 'object'].index\n",
    "all_features[numeric_features] = \\\n",
    "        all_features[numeric_features].apply(lambda x: (x - x.mean()) / (x.std()))\n",
    "\n",
    "# After standardizing the data all means vanish, hence we can set missing values to 0\n",
    "all_features[numeric_features] = all_features[numeric_features].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network \n",
    "from mxnet import np, npx\n",
    "from mxnet.gluon import nn\n",
    "npx.set_np()\n",
    "\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(256, activation='relu'))\n",
    "net.add(nn.Dense(10))\n",
    "net.initialize()\n",
    "X = np.random.uniform(size=(2, 20))\n",
    "print(X)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = nn.Dense(256, activation='relu')\n",
    "        self.out = nn.Dense(10)\n",
    "    def forward(self, X):\n",
    "        return self.out(self.hidden(X))\n",
    "    \n",
    "class MySequential(nn.Block):\n",
    "    def add(self, block):\n",
    "        self._children[block.name] = block\n",
    "    def forward(self, X):\n",
    "        for block in self._children.values():\n",
    "            X = block(X)\n",
    "        return X\n",
    "    \n",
    "#test 1\n",
    "neqt = MySequential()\n",
    "neqt.add(nn.Dense(256, activation='relu'))\n",
    "neqt.add(nn.Dense(10))\n",
    "neqt.initialize()\n",
    "neqt(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case we want to add constant values, and control flow\n",
    "\n",
    "class FixedHiddenMLP(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # Random weight parameters created with the `get_constant` function\n",
    "        # are not updated during training (i.e., constant parameters)\n",
    "        self.rand_weight = self.params.get_constant('rand_weight', np.random.uniform(size=(20, 20)))\n",
    "        self.dense = nn.Dense(20, activation='relu')\n",
    "    def forward(self, X):\n",
    "        X = self.dense(X)\n",
    "        # Use the created constant parameters, as well as the `relu` and `dot`\n",
    "        X = npx.relu(np.dot(X, self.rand_weight.data()) + 1)\n",
    "        # Reuse the fully-connected layer. This is equivalent to sharing\n",
    "        # parameters with two fully-connected layers\n",
    "        X = self.dense(X)\n",
    "        # Control flow\n",
    "        while np.abs(X).sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()\n",
    "# test 1\n",
    "FNN = FixedHiddenMLP()\n",
    "FNN.initialize()\n",
    "FNN(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # task \n",
    "# 2. Implement a block that takes two blocks as an argument, say net1 and net2 and returns\n",
    "# the concatenated output of both networks in the forward propagation. This is also called a\n",
    "# parallel block.\n",
    "from mxnet import np, npx\n",
    "from mxnet.gluon import nn\n",
    "npx.set_np()\n",
    "\n",
    "class MyTwoBlock(nn.Block):\n",
    "    def add(self, net1, net2):\n",
    "        self.register_child(net1, net1.name)\n",
    "        self.register_child(net2, net2.name)\n",
    "\n",
    "    def forward(self, X):\n",
    "        print(self._children.get('one'), self._children.get('two'))\n",
    "        \n",
    "X_ = np.random.uniform(size=(2, 25))\n",
    "\n",
    "nee = MyTwoBlock()\n",
    "nee.add(nn.Dense(256, activation='relu', prefix='one'), nn.Dense(128, activation='relu', prefix='two'))\n",
    "nee.initialize()\n",
    "nee(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters Initializing\n",
    "\n",
    "from mxnet import np, npx, init\n",
    "from mxnet.gluon import nn\n",
    "npx.set_np()\n",
    "\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(10, activation='relu'))\n",
    "net.add(nn.Dense(1))\n",
    "net.initialize()\n",
    "\n",
    "X = np.random.uniform(size=(2, 5))\n",
    "\n",
    "res_X = net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net[0].params, net[1].params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net[0].bias.data()\n",
    "# net[0].weight.grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net.collect_params()['dense4_bias'].data() # one way to have al params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can re inint params with other distr\n",
    "net.initialize(init=init.Normal(sigma=0.01), force_reinit=True)\n",
    "net.collect_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net[0].weight.data()[0] # get the weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.initialize(init=init.Constant(1), force_reinit=True)\n",
    "net.collect_params()\n",
    "net[0].weight.data()[0] # get the weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  init for each block \n",
    "net[0].weight.initialize(init=init.Xavier(), force_reinit=True)\n",
    "\n",
    "net[1].initialize(init=init.Normal(sigma=0.01), force_reinit=True)\n",
    "\n",
    "print(net[0].weight.data()[0])\n",
    "print(net[1].weight.data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.05806115, 0.01261444, 0.02056007, 0.04615813, 0.06212568,\n",
       "        0.03105562, 0.11054351, 0.01731615, 0.        , 0.06471774]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taks from 5.4.1\n",
    "# 1. Design a layer that takes an input and computes a tensor reduction, it returns Yk = Wijk * X i * X j\n",
    "from mxnet import np, npx, init\n",
    "from mxnet.gluon import nn\n",
    "npx.set_np()\n",
    "\n",
    "class LayerTensorReduction(nn.Block):\n",
    "    \n",
    "    def __init__(self, units, in_units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.weight = self.params.get('weight', shape=(in_units, units))\n",
    "        self.bias = self.params.get('bias', shape=(units,))\n",
    "    def forward(self, x):\n",
    "        linear = np.dot(x, self.weight.data(ctx=x.ctx)) + self.bias.data(ctx=x.ctx)\n",
    "        return npx.relu(linear)\n",
    "        \n",
    "model = LayerTensorReduction(10, 4)\n",
    "model.params\n",
    "model.initialize()\n",
    "\n",
    "Y = model.forward(x=np.random.uniform(size=(1, 4)))\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_gpu(i=0):\n",
    "    if npx.num_gpus() >= (i + 1):\n",
    "        return npx.gpu(i)\n",
    "    else:\n",
    "        return npx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2., 3.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = np.array([1,2,3], ctx=try_gpu())\n",
    "npx.gpu(i) \n",
    "xx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2. Convolutions for Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from d2l import mxnet as d2l\n",
    "from mxnet import autograd, np, npx\n",
    "from mxnet.gluon import nn\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19., 25.],\n",
       "       [37., 43.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[0.0, 1.0, 2.0], \\\n",
    "              [3.0, 4.0, 5.0], \\\n",
    "              [6.0, 7.0, 8.0]])\n",
    "K = np.array([[0.0, 1.0], \\\n",
    "              [2.0, 3.0]])\n",
    "reduce_sum = lambda x, *args, **kwargs: x.sum(*args, **kwargs)\n",
    "\n",
    "def convolution_2d(X, K):\n",
    "    h, w = K.shape\n",
    "    Y = np.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "#             print(X[i: i + h, j: j + w])\n",
    "            Y[i, j] = reduce_sum(X[i: i + h, j: j + w] * K)\n",
    "    return Y    \n",
    "convolution_2d(X, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class convolution2d(nn.Block):\n",
    "    \n",
    "    def __init__(self, kernel, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.weight = self.params.get('weight', shape=kernel)\n",
    "        self.bias = self.params.get('bias', shape=(1,))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return convolution_2d(X, self.weight()) + self.bias()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1., 1., 1., 1., 0., 0., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 0., 0., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 0., 0., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 0., 0., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 0., 0., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 0., 0., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 0., 0., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 0., 0., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 0., 0., 1., 1., 1.]]),\n",
       " array([[ 1., -1.]]))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# constract image with edges\n",
    "X = np.ones([9, 9])\n",
    "X[:, 4:6] = 0 # add black \n",
    "K = np.array([[1.0, -1.0]])\n",
    "X, K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  1.,  0., -1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0., -1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0., -1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0., -1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0., -1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0., -1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0., -1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0., -1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0., -1.,  0.,  0.]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convolution_2d(X, K) #find edges "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
